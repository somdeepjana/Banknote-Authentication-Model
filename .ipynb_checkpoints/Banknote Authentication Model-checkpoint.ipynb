{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C92GhaNTF83g",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "<h1 align='center'> TITLE: Banknote Authentication Model </h1>\n",
    "\n",
    "<h2 align='center'> UNIVERSITY OF ENGINEERING & MANAGEMENT KOLKATA </h2>\n",
    "<h3 align='right'> GROUP ID: 28 </h3>\n",
    "<h3 align='right'> CSE_3A_55 - Somdeep Jana - 12017009001009 </h3>\n",
    "<h3 align='right'> CSE_3A_56 - Soumodip Roy - 12017009001422 </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ad353OwIvyMX",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Introduction\n",
    "    This project is created for checking multiple classifiers on the UCI Banknote Authentication Data Set( https://archive.ics.uci.edu/ml/datasets/banknote+authentication).\n",
    "    \n",
    "    Everyday millions of people use banknotes to make transactions. The security of these banknotes for governments and banks is hence an essential factor to fight froud.\n",
    "\n",
    "    Nowadays, sometimes it is too hard to spot counterfeit and genuine notes. Then, the aim of this example is to create a support system ready to help organizations to accurately classify fraudulent notes.\n",
    "\n",
    "    Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels.\n",
    "\n",
    "    Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.025Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1163,
     "status": "ok",
     "timestamp": 1584000076544,
     "user": {
      "displayName": "Somdeep Jana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXWFWU6LAOctjP_oawRxxAfvdoVHQdiqBCxjxZug=s64",
      "userId": "02840926678953105957"
     },
     "user_tz": -330
    },
    "id": "CT8G8ukVvyMY",
    "outputId": "e0425ac8-75e1-45a4-a61c-982cd0e26d16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"https://archive.ics.uci.edu/ml/datasets/banknote+authentication\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x25311ab5988>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://archive.ics.uci.edu/ml/datasets/banknote+authentication', width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cGXalUYHvyMc",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Objective\n",
    "    Main objective of this project is to evaluate, which classifier provides the best optimal solution for classifying whether a note is authentic or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvmP1y-MF83i",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Importing Necessary Packages\n",
    "    We are going to use python for this project. so first step is to getting all the necessary pckages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.027Z"
    },
    "cell_style": "split",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1159,
     "status": "ok",
     "timestamp": 1584000076546,
     "user": {
      "displayName": "Somdeep Jana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXWFWU6LAOctjP_oawRxxAfvdoVHQdiqBCxjxZug=s64",
      "userId": "02840926678953105957"
     },
     "user_tz": -330
    },
    "id": "P4ACAyhDHFg1",
    "outputId": "d5a80455-f081-4c9b-be37-b1d8be335c17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.16.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.029Z"
    },
    "cell_style": "split",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1155,
     "status": "ok",
     "timestamp": 1584000076546,
     "user": {
      "displayName": "Somdeep Jana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXWFWU6LAOctjP_oawRxxAfvdoVHQdiqBCxjxZug=s64",
      "userId": "02840926678953105957"
     },
     "user_tz": -330
    },
    "id": "SINRGTdOHJCw",
    "outputId": "118ae37f-ac2c-42aa-dc22-5c75fc37de05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # for loading the CSV data\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.031Z"
    },
    "cell_style": "center",
    "colab": {},
    "colab_type": "code",
    "id": "dTpWnp29HBBn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # for spliting data into Training set and Test set\n",
    "from sklearn.model_selection import GridSearchCV # for choosing the hyper paraenter\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics # for evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier # for applying Knn classifier\n",
    "from sklearn import svm # for applying svm\n",
    "from sklearn.neural_network import MLPClassifier # for appling Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.032Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "xrbLOkuRvyMm"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt # for ploting the evaluating metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZgbwbzuFF83n",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Loading & Pre-processing The DataSet\n",
    "    The data set is in CSV(Comma seperated Value) format. We are using pandas for importing this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.035Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1584000076547,
     "user": {
      "displayName": "Somdeep Jana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXWFWU6LAOctjP_oawRxxAfvdoVHQdiqBCxjxZug=s64",
      "userId": "02840926678953105957"
     },
     "user_tz": -330
    },
    "id": "upKNLP5bF83o",
    "outputId": "9695d02e-720b-4627-88ff-6af5c5d5b17c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Curtosis</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variance  Skewness  Curtosis  Entropy  Class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_name= [\"Variance\", \"Skewness\", \"Curtosis\", \"Entropy\", \"Class\"]  # These are the features list for our dataset\n",
    "banknote_data_set= pd.read_csv('data_banknote_authentication.csv', names= features_name)\n",
    "banknote_data_set.head(5) # geting a snapshot of our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "colab_type": "text",
    "id": "-H19h3H9IMvK"
   },
   "source": [
    "## Non-Numeric Values\n",
    "    Our dataset dosen't contain any Non-Numeric value so we don't have to worry about that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "colab_type": "text",
    "id": "G4VclPdMLTiY"
   },
   "source": [
    "## Null Value\n",
    "    Our dataset dose't have any null value so we don't have to wory about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "colab_type": "text",
    "id": "DP1iV5zDMds8"
   },
   "source": [
    "## Standardization & Normalization\n",
    "    Our dataset contains continious values so the values in all the columns are not in same range so we are performing Normalization and the scale across all the data are also not same so one value may get uncondition prority than others thats why we are performing Standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.038Z"
    },
    "cell_style": "center",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1584000076548,
     "user": {
      "displayName": "Somdeep Jana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXWFWU6LAOctjP_oawRxxAfvdoVHQdiqBCxjxZug=s64",
      "userId": "02840926678953105957"
     },
     "user_tz": -330
    },
    "id": "NjLKeL3RSwJz",
    "outputId": "9e4b5c4b-1882-4c54-db82-a02a7c401122"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\tMean                \tStd       \n",
      " 1\t0.4337352570699707\t2.842762586278562\n",
      " 2\t1.9223531206393603\t5.869046743695522\n",
      " 3\t1.3976271172667651\t4.310030090106595\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Curtosis</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.121397</td>\n",
       "      <td>1.149036</td>\n",
       "      <td>-0.975614</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.446538</td>\n",
       "      <td>1.064065</td>\n",
       "      <td>-0.894710</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.207369</td>\n",
       "      <td>-0.777069</td>\n",
       "      <td>0.122174</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.063355</td>\n",
       "      <td>1.295005</td>\n",
       "      <td>-1.254940</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.036758</td>\n",
       "      <td>-1.086642</td>\n",
       "      <td>0.736462</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variance  Skewness  Curtosis  Entropy  Class\n",
       "0  1.121397  1.149036 -0.975614 -0.44699      0\n",
       "1  1.446538  1.064065 -0.894710 -1.46210      0\n",
       "2  1.207369 -0.777069  0.122174  0.10645      0\n",
       "3  1.063355  1.295005 -1.254940 -3.59440      0\n",
       "4 -0.036758 -1.086642  0.736462 -0.98880      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"{0:2}\\t{1:20}\\t{2:10}\".format(\"No\", \"Mean\", \"Std\"))\n",
    "for column_name in features_name[0:3]:  # for all the columns except the last one\n",
    "  column_mean= banknote_data_set[column_name].mean()\n",
    "  print(\"{0:2d}\\t{1}\".format( features_name.index(column_name) + 1, column_mean), end=\"\\t\")\n",
    "\n",
    "  column_std= banknote_data_set[column_name].std()\n",
    "  print(\"{}\".format(column_std))\n",
    "\n",
    "  banknote_data_set[column_name]= (banknote_data_set[column_name]-column_mean)/column_std\n",
    "\n",
    "banknote_data_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "colab_type": "text",
    "id": "rkyiCFt_LYZ8"
   },
   "source": [
    "## Spliting the features and class colum\n",
    "    The imported data have all it's values in the same data frame table witch is hard to work with, so we are spliting it into two parts one is for features an another is for output classes.\n",
    "\n",
    "## Converting The Data Frame to ndArray\n",
    "    Working with pandas data frame is problematic and it may produce inconcistency with sklearn packages so we are converting the Data Frame to ndArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.040Z"
    },
    "cell_style": "center",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1584000076900,
     "user": {
      "displayName": "Somdeep Jana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXWFWU6LAOctjP_oawRxxAfvdoVHQdiqBCxjxZug=s64",
      "userId": "02840926678953105957"
     },
     "user_tz": -330
    },
    "id": "pvm-5eL-Kmi0",
    "outputId": "1a9702df-d409-4a22-b7be-d5b543625e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features ndArray:  [[ 1.12139676  1.14903615 -0.97561433]\n",
      " [ 1.44653822  1.06406494 -0.89471002]\n",
      " [ 1.20736947 -0.7770688   0.12217383]\n",
      " ...\n",
      " [-1.47182015 -2.62069017  3.75764729]\n",
      " [-1.40617978 -1.75583081  2.55111279]\n",
      " [-1.04674069 -0.43966137  0.29850671]]\n",
      "\n",
      "Classes ndArray:  [0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "features= banknote_data_set[features_name[0:3]].to_numpy()\n",
    "classes= banknote_data_set[features_name[-1]].to_numpy()\n",
    "\n",
    "print(\"Features ndArray: \", features, end=\"\\n\\n\")\n",
    "print(\"Classes ndArray: \", classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0DgYa3bVwrk8"
   },
   "source": [
    "## Train-Test Set Split\n",
    "    To evaluate how the classifiers perform on the priviously unseen data we are going to split our original dataset into two sets one for training purpose and another for testing purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-12T07:38:17.041Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1482,
     "status": "ok",
     "timestamp": 1584000076900,
     "user": {
      "displayName": "Somdeep Jana",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXWFWU6LAOctjP_oawRxxAfvdoVHQdiqBCxjxZug=s64",
      "userId": "02840926678953105957"
     },
     "user_tz": -330
    },
    "id": "RXyOciMLvyM4",
    "outputId": "461e4b2c-c4ab-4a9f-e83a-5c82982eff36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Features: \n",
      "\n",
      " [[-1.05666061 -1.49386323  1.7638561 ]\n",
      " [ 1.53469191  0.86275457 -0.7875414 ]\n",
      " [ 1.22516905  1.41875627 -1.2738257 ]\n",
      " ...\n",
      " [ 1.33295153 -0.13468169 -0.1567523 ]\n",
      " [ 0.42686109 -0.24539302  0.77662402]\n",
      " [ 0.35675323 -1.15891957  1.61139777]]\n",
      "\n",
      "Train Classes: \n",
      "\n",
      " [1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0\n",
      " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1\n",
      " 1 1 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 1 0 0 1\n",
      " 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
      " 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 1 0 0 0 1\n",
      " 0 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 0\n",
      " 1 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 1 0\n",
      " 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
      " 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1\n",
      " 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1\n",
      " 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 0 0 1 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0\n",
      " 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1\n",
      " 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0 1 0 0 0]\n",
      "\n",
      "Test Features: \n",
      "\n",
      " [[ 1.24645117  1.572478   -1.2354501 ]\n",
      " [-0.42797287  1.60808855 -0.41161363]\n",
      " [ 1.05607298 -0.21471854  0.16391368]\n",
      " ...\n",
      " [-0.6731956  -2.12074527  1.97677805]\n",
      " [ 1.20107277  1.40253558 -1.2189537 ]\n",
      " [ 1.26491912  0.12834229 -0.1370703 ]]\n",
      "\n",
      "Test Classes: \n",
      "\n",
      " [0 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1\n",
      " 0 1 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 1\n",
      " 1 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1\n",
      " 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0\n",
      " 1 0 1 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 0 1\n",
      " 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0\n",
      " 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0\n",
      " 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1\n",
      " 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 1\n",
      " 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "train_features, test_features, train_classes, test_classes= train_test_split(features, classes, test_size= 0.3)\n",
    "\n",
    "print(\"Train Features: \\n\\n\", train_features, end=\"\\n\\n\")\n",
    "print(\"Train Classes: \\n\\n\", train_classes, end=\"\\n\\n\")\n",
    "print(\"Test Features: \\n\\n\", test_features, end=\"\\n\\n\")\n",
    "print(\"Test Classes: \\n\\n\", test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "somjgEDuvyM7"
   },
   "source": [
    "# K-Nearest Neighbour\n",
    "    KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7mB1Y5xvyM8"
   },
   "source": [
    "## Working Principle\n",
    "    In KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case.\n",
    "    \n",
    "![KNN](img/Knn_k1_z96jba.png)\n",
    "    \n",
    "    Suppose P1 is the point, for which label needs to predict. First, you find the k closest point to P1 and then classify points by majority vote of its k neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, you find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance. KNN has the following basic steps:\n",
    "    \n",
    "    1. Calculate distance\n",
    "    2. Find closest neighbors\n",
    "    3. Vote for labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8DpWIOSzvyM8"
   },
   "source": [
    "## Creating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 960\n",
      "No of K Neighbors: 479\n"
     ]
    }
   ],
   "source": [
    "test_set_no,_ = train_features.shape # geting Training set size\n",
    "print(\"Test set size:\", test_set_no)\n",
    "\n",
    "x_neighbors= list(range(1, (test_set_no//2))) # List of all the possible K values\n",
    "print(\"No of K Neighbors:\", len(x_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating The Best Value of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 479 candidates, totalling 1437 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 328 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 828 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1414 out of 1437 | elapsed:   13.8s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1437 out of 1437 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                            metric='minkowski',\n",
       "                                            metric_params=None, n_jobs=-1,\n",
       "                                            n_neighbors=5, p=2,\n",
       "                                            weights='uniform'),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                         13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "                                         23, 24, 25, 26, 27, 28, 29, 30, ...]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_params= {\n",
    "    'n_neighbors':x_neighbors\n",
    "}\n",
    "\n",
    "knn_model= KNeighborsClassifier(n_jobs=-1)\n",
    "knn_grid= GridSearchCV(knn_model, knn_params, n_jobs= -1, verbose=True)\n",
    "\n",
    "knn_grid.fit(train_features, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 2}\n"
     ]
    }
   ],
   "source": [
    "print(knn_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "    From the above plot we can observe that at ** k=2 ** the accuracy of Training set and Testing set is nearly the same. so finally we are training our model with ** k=2 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model= KNeighborsClassifier(n_neighbors= 2) # creating the classifier object\n",
    "knn_model.fit(train_features, train_classes) # Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Score on the TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       234\n",
      "           1       1.00      1.00      1.00       178\n",
      "\n",
      "    accuracy                           1.00       412\n",
      "   macro avg       1.00      1.00      1.00       412\n",
      "weighted avg       1.00      1.00      1.00       412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_knn= knn_model.predict(test_features) # Testing the model on Test set\n",
    "print(classification_report(test_classes, test_predict_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "    SVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. It is known for its kernel trick to handle nonlinear input spaces.\n",
    "    \n",
    "    SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
    "![SVM](img/index3_souoaz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Terms\n",
    "    There are some basic terms involved with svm. Here we discussedd all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vectors\n",
    "    Support vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperplane\n",
    "    A hyperplane is a decision plane which separates between a set of objects having different class memberships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Margin\n",
    "    A margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Nonlinear data\n",
    "![nonLinear_SVM](img/plot_circle_01.png)\n",
    "\n",
    "    It’s pretty clear that there’s not a linear decision boundary (a single straight line that separates both tags). However, the vectors are very clearly segregated and it looks as though it should be easy to separate them.\n",
    "    \n",
    "    we will add a third dimension. Up until now we had two dimensions: x and y. We create a new z dimension, and we rule that it be calculated a certain way that is convenient for us: z = x² + y²\n",
    "![nonLinear_SVM_3rdDimention](img/plot_circle_02.png)\n",
    "\n",
    "    Note that since we are in three dimensions now, the hyperplane is a plane parallel to the x axis at a certain z (let’s say z = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The kernel trick\n",
    "    The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Kernel\n",
    "    A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n",
    "    \n",
    "    K(x, xi) = sum(x * xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "    A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.\n",
    "    \n",
    "    K(x,xi) = 1 + sum(x * xi)^d\n",
    "    \n",
    "    Where d is the degree of the polynomial. d=1 is similar to the linear transformation. The degree needs to be manually specified in the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function Kernel\n",
    "     RBF can map an input space in infinite dimensional space.\n",
    "     \n",
    "     K(x,xi) = exp(-gamma * sum((x – xi^2))\n",
    "     \n",
    "     Here gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing C values(first 5):  [0.03, 0.06, 0.09, 0.12, 0.15]\n"
     ]
    }
   ],
   "source": [
    "x_c_values= list(np.arange(0, 20, 0.03)[1:])\n",
    "\n",
    "print(\"Testing C values(first 5): \", x_c_values[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5328 candidates, totalling 15984 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  84 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 4584 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 12084 tasks      | elapsed:    5.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 15984 out of 15984 | elapsed:    7.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=True),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'C': [0.03, 0.06, 0.09, 0.12, 0.15, 0.18, 0.21, 0.24,\n",
       "                               0.27, 0.3, 0.32999999999999996, 0.36, 0.39, 0.42,\n",
       "                               0.44999999999999996, 0.48, 0.51, 0.54, 0.57, 0.6,\n",
       "                               0.63, 0.6599999999999999, 0.69, 0.72, 0.75, 0.78,\n",
       "                               0.8099999999999999, 0.84, 0.87,\n",
       "                               0.8999999999999999, ...],\n",
       "                         'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_params= {\n",
    "    'C':x_c_values,\n",
    "    'kernel':[ 'linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma':['scale', 'auto'],\n",
    "}\n",
    "\n",
    "svm_model= svm.SVC(verbose= True)\n",
    "svm_grid= GridSearchCV(svm_model, svm_params, n_jobs= -1, verbose=True)\n",
    "\n",
    "svm_grid.fit(train_features, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 11.16, 'gamma': 'scale', 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(svm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "    From the above plot we can observe that at c=2.04 the accuracy of Training set and Testing set is nearly the same. so finally we are training our model with ** k=2.04 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=11.16, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model= svm.SVC(C=11.16, kernel='rbf', gamma= 'scale') # Creating SVM object\n",
    "svm_model.fit(train_features, train_classes) # Training the model on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Accuracy on TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       234\n",
      "           1       1.00      1.00      1.00       178\n",
      "\n",
      "    accuracy                           1.00       412\n",
      "   macro avg       1.00      1.00      1.00       412\n",
      "weighted avg       1.00      1.00      1.00       412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_svm= svm_model.predict(test_features) # Predicting on Test set\n",
    "print(classification_report(test_classes, test_predict_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "    Neural networks are created by adding the layers of these perceptrons together, known as a multi-layer perceptron model. There are three layers of a neural network - the input, hidden, and output layers. The input layer directly receives the data, whereas the output layer creates the required output. The layers in between are known as hidden layers where the intermediate computation takes place.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward\n",
    "    Following are the steps performed during the feed-forward phase:\n",
    "    \n",
    "    1. The values received in the input layer are multiplied with the weights. A bias is added to the summation of the inputs and weights in order to avoid null values.\n",
    "    \n",
    "    2. Each neuron in the first hidden layer receives different values from the input layer depending upon the weights and bias. Neurons have an activation function that operates upon the value received from the input layer. The activation function can be of many types, like a step function, sigmoid function, relu function, or tanh function. As a rule of thumb, relu function is used in the hidden layer neurons and sigmoid function is used for the output layer neuron.\n",
    "    \n",
    "    3. The outputs from the first hidden layer neurons are multiplied with the weights of the second hidden layer; the results are summed together and passed to the neurons of the proceeding layers. This process continues until the outer layer is reached. The values calculated at the outer layer are the actual outputs of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "\n",
    "![Neural_Network](img/ann-in-hidden-out.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "    Back propagation phase consists of the following steps:\n",
    "    \n",
    "![Back_Propagation_Step](img/tikz21.png)\n",
    "\n",
    "    The backpropagation equations provide us with a way of computing the gradient of the cost function. Let's explicitly write this out in the form of an algorithm:\n",
    "\n",
    "**Input x:** Set the corresponding activation a1 for the input layer.\n",
    "\n",
    "**Feedforward:** For each l=2,3,…,L compute \n",
    "\n",
    "                    zl=wlal−1+bl and al=σ(zl)\n",
    "                    \n",
    "**Output error δL:** Compute the vector\n",
    "\n",
    "                    δL=∇aC⊙σ′(zL)\n",
    "                    \n",
    "**Backpropagate the error:** For each l=L−1,L−2,…,2 compute \n",
    "\n",
    "                    δl=((wl+1)Tδl+1)⊙σ′(zl)\n",
    "                    \n",
    "**Output:** The gradient of the cost function is given by \n",
    "\n",
    "                    ∂C∂wljk=al−1kδlj and ∂C∂blj=δlj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 324 out of 324 | elapsed:   10.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                     batch_size='auto', beta_1=0.9,\n",
       "                                     beta_2=0.999, early_stopping=False,\n",
       "                                     epsilon=1e-08, hidden_layer_sizes=(100,),\n",
       "                                     learning_rate='constant',\n",
       "                                     learning_rate_init=0.001, max_iter=200,\n",
       "                                     momentum=0.9, n_iter_no_change=10,\n",
       "                                     nesterovs_momentum=True, power_t=0.5,\n",
       "                                     rando...\n",
       "                                     validation_fraction=0.1, verbose=True,\n",
       "                                     warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
       "                         'hidden_layer_sizes': [(100, 20), (20, 20), (15,)],\n",
       "                         'learning_rate': ['constant', 'invscaling',\n",
       "                                           'adaptive'],\n",
       "                         'solver': ['lbfgs', 'sgd', 'adam']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params= {\n",
    "    'activation':['identity', 'logistic', 'tanh', 'relu'],\n",
    "    'solver':[ 'lbfgs', 'sgd', 'adam'],\n",
    "    'learning_rate':['constant', 'invscaling', 'adaptive'],\n",
    "    'hidden_layer_sizes':[(100, 20), (20, 20), (15,)]\n",
    "}\n",
    "\n",
    "nn_model= MLPClassifier(verbose= True)\n",
    "nn_grid= GridSearchCV(nn_model, nn_params, n_jobs= -1, verbose=True)\n",
    "\n",
    "nn_grid.fit(train_features, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'logistic', 'hidden_layer_sizes': (100, 20), 'learning_rate': 'adaptive', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "print(nn_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100, 20), learning_rate='adaptive',\n",
       "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model= MLPClassifier(verbose= True, hidden_layer_sizes= (100,20), activation= 'logistic', solver= 'lbfgs', learning_rate='adaptive') # Creating neural network object\n",
    "nn_model.fit(train_features, train_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy on TestSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       234\n",
      "           1       1.00      1.00      1.00       178\n",
      "\n",
      "    accuracy                           1.00       412\n",
      "   macro avg       1.00      1.00      1.00       412\n",
      "weighted avg       1.00      1.00      1.00       412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predict_nn= nn_model.predict(test_features)\n",
    "print(classification_report(test_classes, test_predict_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "    After analyzing various techniques used to detect forged banknotes, this paper presents banknote authentication for recognizing the banknote as genuine or fake by using three supervised learning techniques. Extensive experiments have been performed on banknotes dataset using both the models to find the best model suitable for classification of the notes.\n",
    "    \n",
    "    From this discussion we can see that all the models are perfect interms of accuracy but the neural network model perform best incase of testing speed but training speed of neural network is slow where in KNN we don't have any training phase. SVM is inoverall the best sutable classifier for this type of data because it have a proper balance between training speed and testing speed."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jvmP1y-MF83i"
   ],
   "name": "Banknote Authentication Model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "432px",
    "left": "1551px",
    "right": "20px",
    "top": "117px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
